# Analyse des logs syslog avec IA

## Objectif

Analyser efficacement les logs système Linux (syslog, auth.log, kern.log) pour identifier rapidement les erreurs critiques, les tentatives d'intrusion et les anomalies, en exploitant l'IA pour interpréter des logs volumineux.

## Contexte entreprise

Un administrateur Linux gère une infrastructure de 12 serveurs Ubuntu/Debian :
- 4 serveurs web (Nginx)
- 3 serveurs applicatifs (Node.js, Python)
- 2 serveurs de bases de données (PostgreSQL)
- 3 serveurs de services (DNS, monitoring, backup)

**Volume de logs** : ~500 MB/jour répartis sur tous les serveurs

**Problématiques** :
- Erreurs critiques noyées dans des milliers de lignes
- Messages d'erreur Linux peu clairs
- Tentatives de connexion suspectes difficiles à détecter
- Temps perdu à grep manuellement chaque serveur

**Objectif** : Identifier rapidement les problèmes et agir avant impact utilisateurs.

## Problème

Analyser manuellement les logs Linux implique :
- Se connecter SSH sur chaque serveur
- Utiliser grep/awk/tail pour filtrer
- Interpréter des messages système cryptiques
- Corréler des événements entre plusieurs serveurs
- Rechercher sur Google les erreurs inconnues

**Temps estimé** : 30-60 minutes pour analyser les logs de tous les serveurs

**Difficultés** :
- Volume écrasant d'événements
- Messages techniques peu explicites
- Logs éparpillés sur plusieurs serveurs
- Difficulté à prioriser les erreurs

## Solution traditionnelle

### Méthode manuelle

```bash
# Se connecter à chaque serveur
ssh admin@serveur-web-01

# Consulter les logs syslog
sudo tail -f /var/log/syslog

# Filtrer les erreurs
sudo grep -i error /var/log/syslog | tail -50

# Consulter les tentatives de connexion
sudo grep "Failed password" /var/log/auth.log

# Rechercher des patterns spécifiques
sudo awk '/error|critical|fail/i' /var/log/syslog
```

### Limites

- Processus répétitif sur chaque serveur
- Difficulté à corréler des événements
- Messages d'erreur cryptiques
- Pas de vision d'ensemble

## Apport de l'IA

L'IA permet de :
- Analyser des milliers de lignes de logs en quelques secondes
- Expliquer en langage clair des messages système cryptiques
- Identifier des patterns d'attaque (brute force, scans)
- Proposer des solutions classées par priorité
- Générer des scripts d'analyse automatisés
- Corréler des événements entre plusieurs serveurs

**Temps avec IA** : 5-15 minutes (extraction + analyse + solution)

**Gain de productivité** : 70-80%

## Mise en œuvre étape par étape

### Étape 1 : Extraction des logs pertinents

#### Script de collecte centralisée

```bash
#!/bin/bash
# collect-logs.sh
# Collecte les logs de tous les serveurs

SERVERS=(
    "192.168.1.10"  # web-01
    "192.168.1.11"  # web-02
    "192.168.1.20"  # app-01
    "192.168.1.30"  # db-01
)

OUTPUT_DIR="./logs_$(date +%Y%m%d)"
mkdir -p $OUTPUT_DIR

for server in "${SERVERS[@]}"; do
    echo "Collecte logs depuis $server..."

    # Syslog (dernières 1000 lignes)
    ssh admin@$server "sudo tail -1000 /var/log/syslog" > "$OUTPUT_DIR/${server}_syslog.log"

    # Auth.log (tentatives connexion)
    ssh admin@$server "sudo tail -1000 /var/log/auth.log" > "$OUTPUT_DIR/${server}_auth.log"

    # Nginx errors (si serveur web)
    ssh admin@$server "sudo tail -500 /var/log/nginx/error.log" 2>/dev/null > "$OUTPUT_DIR/${server}_nginx_error.log"

    echo "✓ $server"
done

echo "Logs collectés dans $OUTPUT_DIR"
```

#### Extraction des erreurs critiques

```bash
#!/bin/bash
# extract-errors.sh
# Extrait les erreurs et avertissements de tous les logs

LOG_DIR="./logs_20240211"
OUTPUT="errors_summary.txt"

echo "=== ERREURS CRITIQUES ===" > $OUTPUT

for logfile in $LOG_DIR/*.log; do
    echo -e "\n--- $(basename $logfile) ---" >> $OUTPUT

    # Filtrer erreurs et critiques
    grep -iE "error|critical|fail|panic|segfault" $logfile | tail -20 >> $OUTPUT
done

echo "Résumé généré : $OUTPUT"
```

### Étape 2 : Analyse avec l'IA

**Exemple de logs collectés** (auth.log) :

```
Feb 11 14:23:15 web-01 sshd[12345]: Failed password for root from 103.x.x.x port 54321
Feb 11 14:23:17 web-01 sshd[12345]: Failed password for root from 103.x.x.x port 54322
Feb 11 14:23:19 web-01 sshd[12345]: Failed password for root from 103.x.x.x port 54323
Feb 11 14:23:21 web-01 sshd[12345]: Failed password for admin from 103.x.x.x port 54324
Feb 11 14:23:23 web-01 sshd[12345]: Failed password for admin from 103.x.x.x port 54325
Feb 11 15:42:10 web-01 systemd[1]: Failed to start PostgreSQL database server.
Feb 11 16:05:33 web-01 kernel: Out of memory: Kill process 7891 (node) score 892
```

**Prompt structuré** :

```
Analyse ces logs Linux et identifie les problèmes critiques.

**Serveur** : Ubuntu 22.04 LTS (serveur web Nginx + Node.js)

**Logs /var/log/auth.log** :
[copier-coller les logs auth.log]

**Logs /var/log/syslog** :
[copier-coller les logs syslog avec erreurs]

**Questions** :
1. Quelles sont les menaces de sécurité détectées ?
2. Quelles erreurs système nécessitent une action immédiate ?
3. Quelle est la priorité de chaque problème ?
4. Quelles commandes exécuter pour corriger ?
```

### Étape 3 : Analyser la réponse de l'IA

**Exemple de réponse attendue** :

```
## Analyse des logs Linux - Serveur web-01

### Problèmes identifiés

#### 1. Attaque par force brute SSH (CRITIQUE)

**Logs** :
```
Failed password for root from 103.x.x.x port 54321
Failed password for root from 103.x.x.x port 54322
Failed password for admin from 103.x.x.x port 54324
```

**Analyse** :
5 tentatives de connexion échouées en 10 secondes depuis la même IP (103.x.x.x).
Pattern typique d'attaque par force brute automatisée (bot).

**Cibles** :
- Compte root (interdit en SSH normalement)
- Compte admin (compte courant)

**Danger** :
Si le mot de passe admin est faible, le compte peut être compromis.

**Actions immédiates** :

1. Bloquer l'IP attaquante :
```bash
sudo ufw deny from 103.x.x.x
```

2. Vérifier si l'IP a réussi à se connecter :
```bash
sudo grep "Accepted password" /var/log/auth.log | grep "103.x.x.x"
```
Si résultat → Le compte est compromis, changer immédiatement le mot de passe.

3. Installer fail2ban pour blocage automatique :
```bash
sudo apt install fail2ban -y
sudo systemctl enable fail2ban
sudo systemctl start fail2ban
```

4. Configuration fail2ban :
```bash
sudo nano /etc/fail2ban/jail.local
```

Ajouter :
```ini
[sshd]
enabled = true
port = ssh
logpath = /var/log/auth.log
maxretry = 3
bantime = 3600
findtime = 600
```

Redémarrer :
```bash
sudo systemctl restart fail2ban
```

5. Vérifier les IPs bannies :
```bash
sudo fail2ban-client status sshd
```

---

#### 2. PostgreSQL refuse de démarrer (ÉLEVÉ)

**Log** :
```
Failed to start PostgreSQL database server.
```

**Impact** :
Si PostgreSQL est arrêté, les applications dépendantes (backend, API) ne fonctionnent plus.

**Diagnostic** :

1. Vérifier le statut du service :
```bash
sudo systemctl status postgresql
```

2. Consulter les logs PostgreSQL :
```bash
sudo tail -50 /var/log/postgresql/postgresql-14-main.log
```

**Causes probables** :
- Espace disque saturé
- Fichiers de configuration corrompus
- Port déjà utilisé
- Permissions incorrectes sur les fichiers de données

**Actions** :

1. Vérifier l'espace disque :
```bash
df -h
```
Si < 10% d'espace libre → Libérer de l'espace (logs anciens, fichiers temporaires)

2. Tenter de redémarrer :
```bash
sudo systemctl restart postgresql
sudo systemctl status postgresql
```

3. Si échec, vérifier les permissions :
```bash
sudo ls -la /var/lib/postgresql/14/main
# Propriétaire doit être postgres:postgres
```

4. Corriger si nécessaire :
```bash
sudo chown -R postgres:postgres /var/lib/postgresql/14/main
sudo chmod 700 /var/lib/postgresql/14/main
```

---

#### 3. Out of Memory - Processus Node.js tué (ÉLEVÉ)

**Log** :
```
kernel: Out of memory: Kill process 7891 (node) score 892
```

**Analyse** :
Le système a manqué de RAM et a tué le processus Node.js (PID 7891) pour libérer de la mémoire.
Score 892/1000 indique que ce processus consommait 89% de la RAM disponible.

**Impact** :
Application Node.js arrêtée brutalement → Utilisateurs ne peuvent plus accéder au service.

**Causes probables** :
- Memory leak dans l'application Node.js
- Serveur sous-dimensionné en RAM
- Pic de charge inhabituel

**Actions immédiates** :

1. Redémarrer l'application Node.js :
```bash
sudo systemctl restart mon-app-node
# ou si géré par PM2 :
pm2 restart all
```

2. Vérifier la RAM disponible :
```bash
free -h
# Regarder "available"
```

3. Identifier les processus consommant le plus de RAM :
```bash
ps aux --sort=-%mem | head -10
```

4. Monitorer en temps réel :
```bash
htop
```

**Actions à moyen terme** :

1. Analyser l'application pour memory leaks (code review, profiling)

2. Augmenter la RAM du serveur (si sous-dimensionné)

3. Configurer un monitoring avec alertes (Prometheus, Grafana) :
```bash
# Alerte si RAM > 80% pendant 5 minutes
```

4. Configurer swap si absent :
```bash
# Vérifier si swap existe
swapon --show

# Créer un fichier swap de 4 GB si absent
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# Rendre permanent
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

---

### Résumé des priorités

| Problème | Criticité | Impact | Action immédiate |
|----------|-----------|--------|------------------|
| Attaque force brute SSH | CRITIQUE | Compromission compte | Bloquer IP + fail2ban |
| PostgreSQL down | ÉLEVÉ | Applications offline | Redémarrer PostgreSQL |
| OOM Node.js | ÉLEVÉ | Service interrompu | Redémarrer app + monitoring |

### Script de vérification rapide

```bash
#!/bin/bash
# health-check.sh
# Vérifie l'état de santé du serveur

echo "=== HEALTH CHECK ==="

# 1. Attaques SSH
echo -e "\n[SSH] Tentatives échouées (dernière heure) :"
sudo grep "Failed password" /var/log/auth.log | grep "$(date +%b\ %d)" | wc -l

# 2. Services critiques
echo -e "\n[SERVICES] État :"
for service in nginx postgresql; do
    if systemctl is-active --quiet $service; then
        echo "✓ $service : actif"
    else
        echo "✗ $service : INACTIF"
    fi
done

# 3. RAM disponible
echo -e "\n[RAM] :"
free -h | grep Mem

# 4. Espace disque
echo -e "\n[DISQUE] :"
df -h | grep -E "/$|/var"

# 5. Charge système
echo -e "\n[LOAD] :"
uptime
```
```

### Étape 4 : Appliquer les corrections

Suivre les commandes proposées par l'IA, en commençant par la criticité la plus élevée.

#### Correction 1 : Bloquer attaque SSH

```bash
# Bloquer l'IP
sudo ufw deny from 103.x.x.x

# Installer fail2ban
sudo apt install fail2ban -y

# Configurer
sudo tee /etc/fail2ban/jail.local <<EOF
[sshd]
enabled = true
port = ssh
logpath = /var/log/auth.log
maxretry = 3
bantime = 3600
findtime = 600
EOF

sudo systemctl restart fail2ban
```

#### Correction 2 : Redémarrer PostgreSQL

```bash
# Vérifier l'espace disque
df -h

# Redémarrer
sudo systemctl restart postgresql

# Vérifier le statut
sudo systemctl status postgresql
```

#### Correction 3 : Monitorer la RAM

```bash
# Créer swap si absent
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

### Étape 5 : Automatiser l'analyse quotidienne

**Script d'analyse automatisée avec IA (via API)** :

```bash
#!/bin/bash
# daily-log-analysis.sh
# Analyse quotidienne des logs avec IA

# Variables
API_KEY="votre-cle-api-openai"
LOG_EXTRACT="/tmp/logs_extract.txt"
REPORT="/var/log/daily_analysis_$(date +%Y%m%d).txt"

# Extraction des erreurs des dernières 24h
sudo journalctl --since "24 hours ago" --priority=err > $LOG_EXTRACT

# Appel à l'API OpenAI (ou Claude)
curl -X POST "https://api.openai.com/v1/chat/completions" \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d "{
    \"model\": \"gpt-4\",
    \"messages\": [{
      \"role\": \"user\",
      \"content\": \"Analyse ces logs Linux et identifie les problèmes critiques : $(cat $LOG_EXTRACT)\"
    }]
  }" | jq -r '.choices[0].message.content' > $REPORT

# Envoi du rapport par email
mail -s "Rapport d'analyse logs quotidien" admin@entreprise.com < $REPORT

echo "Rapport généré : $REPORT"
```

**Planification avec cron** :

```bash
# Exécuter tous les jours à 7h00
0 7 * * * /root/scripts/daily-log-analysis.sh
```

## Scripts / Commandes / Configurations

### Script de monitoring en temps réel

```bash
#!/bin/bash
# monitor-logs-realtime.sh
# Surveillance en temps réel avec alertes

echo "=== Monitoring logs en temps réel ==="

tail -f /var/log/syslog /var/log/auth.log | while read line; do
    # Détection tentatives SSH échouées
    if echo "$line" | grep -q "Failed password"; then
        IP=$(echo "$line" | grep -oP '(\d{1,3}\.){3}\d{1,3}')
        echo "[ALERTE SSH] Tentative échouée depuis $IP"

        # Bloquer après 5 tentatives
        COUNT=$(grep "$IP" /var/log/auth.log | grep "Failed password" | wc -l)
        if [ $COUNT -gt 5 ]; then
            sudo ufw deny from $IP
            echo "[ACTION] IP $IP bloquée"
        fi
    fi

    # Détection erreurs critiques
    if echo "$line" | grep -iE "critical|panic|segfault"; then
        echo "[ALERTE CRITIQUE] $line"
        # Envoyer notification (Slack, email, SMS)
    fi

    # Détection OOM
    if echo "$line" | grep -q "Out of memory"; then
        PROCESS=$(echo "$line" | grep -oP 'Kill process \d+ \(\K[^)]+')
        echo "[ALERTE RAM] OOM - Processus $PROCESS tué"
    fi
done
```

### Commandes essentielles d'analyse de logs

```bash
# Consulter les logs système en temps réel
sudo journalctl -f

# Logs des dernières 24h avec priorité erreur
sudo journalctl --since "24 hours ago" --priority=err

# Logs d'un service spécifique (ex: Nginx)
sudo journalctl -u nginx --since today

# Filtrer par mot-clé
sudo journalctl | grep -i "error"

# Afficher les boots récents
sudo journalctl --list-boots

# Logs d'un boot spécifique
sudo journalctl -b -1  # -1 = boot précédent

# Filtrer par période
sudo journalctl --since "2024-02-11 14:00" --until "2024-02-11 16:00"

# Top 10 des erreurs les plus fréquentes
sudo journalctl --priority=err | awk '{print $5}' | sort | uniq -c | sort -nr | head -10

# Tentatives de connexion SSH échouées (dernières 24h)
sudo grep "Failed password" /var/log/auth.log | grep "$(date +%b\ %d)"

# Connexions SSH réussies
sudo grep "Accepted password" /var/log/auth.log

# Erreurs Nginx
sudo tail -f /var/log/nginx/error.log

# Analyser les permissions refusées (SELinux/AppArmor)
sudo grep "Permission denied" /var/log/syslog
```

### Configuration rsyslog centralisé

Pour collecter automatiquement les logs de tous les serveurs sur un serveur central :

**Sur le serveur central (collecteur)** :

```bash
# /etc/rsyslog.conf
# Activer la réception UDP et TCP
module(load="imudp")
input(type="imudp" port="514")

module(load="imtcp")
input(type="imtcp" port="514")

# Template pour séparer les logs par hôte
$template RemoteLogs,"/var/log/remote/%HOSTNAME%/%PROGRAMNAME%.log"
*.* ?RemoteLogs
```

Redémarrer :
```bash
sudo systemctl restart rsyslog
```

**Sur chaque serveur client** :

```bash
# /etc/rsyslog.conf
# Envoyer tous les logs au serveur central
*.* @@192.168.1.100:514  # TCP (plus fiable)
# ou
*.* @192.168.1.100:514   # UDP (plus rapide)
```

Redémarrer :
```bash
sudo systemctl restart rsyslog
```

## Risques et limites

### Risques de diagnostic erroné

**Faux positifs** : Certaines erreurs peuvent sembler critiques mais être normales (ex: tentatives de connexion échouées légitimes d'utilisateurs ayant oublié leur mot de passe).

**Corrélation manquante** : L'IA analyse les logs fournis, mais ne peut pas corréler automatiquement avec d'autres sources (métriques système, logs applicatifs).

### Limites de l'IA

**Pas d'accès direct aux serveurs** : L'IA ne peut pas se connecter aux serveurs ni exécuter de commandes. Elle dépend des extractions fournies.

**Logs spécifiques** : Pour des applications custom, l'IA peut manquer de contexte.

### Risques opérationnels

**Blocage d'IPs légitimes** : Un script automatique qui bloque des IPs peut verrouiller un admin légitime.

**Modifications sans backup** : Toujours sauvegarder les configurations avant modifications.

## Bonnes pratiques

### Analyse régulière

1. Analyser quotidiennement les logs critiques (auth.log, syslog)
2. Automatiser l'extraction des erreurs (cron job)
3. Centraliser les logs sur un serveur dédié (rsyslog)
4. Archiver les anciens logs (rotation avec logrotate)

### Sécurité

1. Installer fail2ban sur tous les serveurs exposés
2. Désactiver SSH root login (`PermitRootLogin no`)
3. Changer le port SSH (réduire le bruit)
4. Utiliser des clés SSH au lieu de mots de passe

### Monitoring

1. Mettre en place des alertes sur erreurs critiques
2. Monitorer les métriques système (RAM, CPU, disque)
3. Créer des dashboards (Grafana + Prometheus)
4. Tester régulièrement les alertes

### Documentation

1. Documenter les erreurs récurrentes et leurs solutions
2. Créer une base de connaissances
3. Former l'équipe aux commandes essentielles
4. Maintenir un runbook des incidents courants

## Ce que cela démontre à un recruteur

### Compétences techniques

- Maîtrise des logs Linux (syslog, journalctl, auth.log)
- Connaissance des outils d'analyse (grep, awk, journalctl)
- Capacité à scripter en Bash
- Compréhension de la sécurité Linux (fail2ban, SSH hardening)

### Méthodologie de diagnostic

- Approche structurée : extraction → analyse → correction → validation
- Utilisation de l'IA pour accélérer l'analyse
- Priorisation des problèmes par criticité

### Proactivité

- Mise en place de monitoring automatisé
- Centralisation des logs pour analyse globale
- Automatisation des tâches répétitives
- Documentation des solutions

### Efficacité

- Réduction du temps d'analyse de 70-80%
- Détection rapide des problèmes critiques
- Résolution avant impact utilisateurs

**Message au recruteur** : Ce guide démontre un administrateur Linux capable d'analyser efficacement des volumes importants de logs système en exploitant l'IA pour accélérer le diagnostic. Il ne se contente pas de lire passivement les logs, mais met en place des systèmes de monitoring proactifs, automatise les analyses et documente les résolutions, garantissant une infrastructure stable et sécurisée.
